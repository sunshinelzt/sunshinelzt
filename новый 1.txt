Skip to content

OnlySq Documentation
Main Navigation
Home

API
All models
About us

Sidebar Navigation
OnlySq API
About

Get started

All models

Tools
Image understanding

Endpoints
SDK OpenAI

POST API2.0

POST ImaGen

BETA OnlySq Cloud
How to work with Cloud?

On this page
SDKs
Python
TypeScript
Example response (via the Python SDK)
Working with OnlySq API2.0
OnlySq platform allows you to feel power of LLMs in your projects. Our advanced models allow for their use in any projects, ranging from small ones (chatbot, text analysis) to extended and large ones (virtual assistant, agents). Learn how to work with the models and familiarize yourself with their list.

SDKs
OnlySq supports calls via OpenAI SDK libraries as the primary way of accessing API, providing advanced models for the user and ease of use for the developer. To get started, please see the installation methods and code snippets below.

Python

pip install -U openai

from openai import OpenAI

client = OpenAI(
    base_url="https://api.onlysq.ru/ai/openai",
    api_key="openai",
)

completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {
            "role": "user",
            "content": "Say 5 short facts about AI.",
        },
    ],
)

print(completion.choices[0].message.content)
TypeScript

npm install openai

import OpenAI from "openai";

const openai = new OpenAI({
    baseURL: "https://api.onlysq.ru/ai/openai",
    apiKey: "openai",
    });

const completion = await openai.chat.completions.create({
    model: "gpt-4o-mini",
    messages: [
        {
            role: "user",
            content: "Say 5 short facts about AI.",
        },
    ]
});

console.log(completion.choices[0].message.content);
Example response (via the Python SDK)

Here are five short facts about AI:  

1. **AI Learns from Data** â€“ AI systems improve by analyzing large amounts of data, identifying patterns, and making predictions.          
2. **Narrow vs. General AI** â€“ Most AI today is "narrow" (task-specific), while "general AI" (human-like reasoning) is still theoretical.  
3. **AI Powers Everyday Tech** â€“ Virtual assistants (Siri, Alexa), recommendations (Netflix, Spotify), and spam filters all use AI.        
4. **Ethical Concerns Exist** â€“ AI raises issues like bias in algorithms, job displacement, and privacy risks.  
5. **AI is Evolving Fast** â€“ Breakthroughs in deep learning and generative AI (like ChatGPT) are rapidly advancing the field.  

Would you like more details on any of these?
Pager
Next page
Get started



Skip to content

OnlySq Documentation
Main Navigation
Home

API
All models
About us

Sidebar Navigation
OnlySq API
About

Get started

All models

Tools
Image understanding

Endpoints
SDK OpenAI

POST API2.0

POST ImaGen

BETA OnlySq Cloud
How to work with Cloud?

Get started
Only Sq is a dedicated team of programmers specializing in the development of API2.0, sites, bots and many scripts. The team focuses on integrating advanced neural network models into their API offerings, enhancing the functionality and performance of their services. They prioritize user experience and efficient data processing, ensuring that their API is both powerful and easy to use for developers.

Starting with patch 4.0.1 (November 29, 2025), every request to v2/imagen must have Authorization header


{"Authorization": "Bearer apikey"}
Basic API key is openai

API Reference
Integrate natural language processing and generation into your projects with a few lines of code and internet connection

Get started â†’

OpenAI SDK
SDK is the primary way of accessing OnlySq API models. Look at the code examples and build something cool!

Get started â†’

Pager
Previous page
About
Next page
All models


Skip to content

OnlySq Documentation
Main Navigation
Home

API
All models
About us

Sidebar Navigation
OnlySq API
About

Get started

All models

Tools
Image understanding

Endpoints
SDK OpenAI

POST API2.0

POST ImaGen

BETA OnlySq Cloud
How to work with Cloud?

Overview of OnlySq models
OnlySq offers a wide range of models for various user needs. All models supports "system" role for more customisation. You can get json of models by using:

GET https://api.onlysq.ru/ai/models

ChatGPT models
ChatGPT is a free and easy-to-use app that can help you with writing, learning, brainstorming, and more.

Model name	Description	Type	Modality	Endpoints
gpt-4o-mini	A small model with superior textual intelligence and multimodal reasoning	Keys	Text	API2.0
OpenAI SDK
gpt-4o-mini-2024-07-18	A July 18, 2024 release of GPTâ€‘4o Mini, optimized for speed and low-cost usage while maintaining strong reasoning capabilities for common tasks.	Keys	Text	API2.0
OpenAI SDK
gpt-4o-2024-05-13	A May 13, 2024 version of GPTâ€‘4o, providing robust performance for general-purpose and multimodal tasks with improved efficiency.	Keys	Text	API2.0
OpenAI SDK
gpt-4o-2024-08-06	An August 6, 2024 release of GPTâ€‘4o, featuring updates for improved alignment, reasoning, and cost-effectiveness in production scenarios.	Keys	Text	API2.0
OpenAI SDK
gpt-4o-2024-11-20	A November 20, 2024 version of GPTâ€‘4o, tuned for better performance on complex reasoning and long-context conversations.	Keys	Text	API2.0
OpenAI SDK
gpt-4o	GPTâ€‘4o (â€œoâ€ for â€œomniâ€) is a step towards much more natural human-computer interaction	Keys	Text	API2.0
OpenAI SDK
chatgpt-4o-latest	The latest ChatGPT experience powered by GPTâ€‘4o, providing the most up-to-date behaviour for conversational and assistant-style interactions.	Keys	Text	API2.0
OpenAI SDK
gpt-4.1	GPTâ€‘4.1 is a highly capable general-purpose model with strong reasoning, coding, and multilingual abilities, suitable for most production workloads.	Keys	Text	API2.0
OpenAI SDK
gpt-4.1-2025-04-14	An April 14, 2025 release of GPTâ€‘4.1, with refinements for reliability, alignment, and broader domain coverage in real-world scenarios.	Keys	Text	API2.0
OpenAI SDK
gpt-4.1-mini	A compact variant of GPTâ€‘4.1, designed for lower latency and cost while retaining strong performance on everyday tasks.	Keys	Text	API2.0
OpenAI SDK
gpt-4.1-mini-2025-04-14	An April 14, 2025 version of GPTâ€‘4.1 Mini with further optimizations for speed, efficiency, and typical assistant workloads.	Keys	Text	API2.0
OpenAI SDK
gpt-4.1-nano	An ultra-small GPTâ€‘4.1-based model designed for on-device or very low-resource environments and simple assistance tasks.	Keys	Text	API2.0
OpenAI SDK
gpt-4.1-nano-2025-04-14	A refined April 14, 2025 build of GPTâ€‘4.1 Nano, offering improved quality for resource-constrained deployments.	Keys	Text	API2.0
OpenAI SDK
o3-mini	OpenAI o3â€‘mini is a small reasoning model that supports function calling, structured outputs, and developer messages, making it production-ready out of the gate.	Keys	Text	API2.0
OpenAI SDK
o3-mini-2025-01-31	A January 31, 2025 release of o3â€‘mini with improved reasoning stability, speed, and integration for developer workflows.	Keys	Text	API2.0
OpenAI SDK
o3	A larger OpenAI o3 reasoning model focused on highly complex, multi-step problem solving, coding, and analytical tasks.	Keys	Text	API2.0
OpenAI SDK
o3-2025-04-16	An April 16, 2025 build of o3, offering refined reasoning traces and stronger performance on challenging benchmarks.	Keys	Text	API2.0
OpenAI SDK
o4-mini	A next-generation small reasoning model in the oâ€‘series, balancing cost, speed, and advanced reasoning for mainstream applications.	Keys	Text	API2.0
OpenAI SDK
o4-mini-2025-04-16	An April 16, 2025 release of o4â€‘mini, featuring the latest updates in reasoning quality and latency improvements.	Keys	Text	API2.0
OpenAI SDK
o1	An OpenAI reasoning model designed for complex step-by-step problem solving, coding, and analytical work with detailed intermediate reasoning.	Keys	Text	API2.0
OpenAI SDK
o1-2024-12-17	A December 17, 2024 snapshot of o1 with updated capabilities and refinements to reasoning reliability.	Keys	Text	API2.0
OpenAI SDK
gpt-4-turbo	GPTâ€‘4 Turbo is a fast, cost-effective variant of GPTâ€‘4, optimized for conversation and common assistant tasks.	Keys	Text	API2.0
OpenAI SDK
gpt-4-turbo-2024-04-09	An April 9, 2024 release of GPTâ€‘4 Turbo that improves speed, cost, and quality balance for production applications.	Keys	Text	API2.0
OpenAI SDK
gpt-4-turbo-preview	A preview build of GPTâ€‘4 Turbo offering access to the latest experimental features and behaviors before general release.	Keys	Text	API2.0
OpenAI SDK
gpt-4o-search-preview	A GPTâ€‘4o-based search-optimized model preview, tuned for retrieval-augmented generation and search-style tasks.	Keys	Text	API2.0
OpenAI SDK
gpt-4o-search-preview-2025-03-11	A March 11, 2025 release of GPTâ€‘4o Search Preview, with refined ranking, retrieval, and grounded answer generation.	Keys	Text	API2.0
OpenAI SDK
gpt-4o-mini-search-preview	A lightweight GPTâ€‘4o Mini model optimized for search and retrieval-style tasks with lower latency.	Keys	Text	API2.0
OpenAI SDK
gpt-4o-mini-search-preview-2025-03-11	A March 11, 2025 version of GPTâ€‘4o Mini Search Preview offering improved relevance and grounded responses at low cost.	Keys	Text	API2.0
OpenAI SDK
gpt-5	GPTâ€‘5 is a next-generation large language model providing superior reasoning, coding, and multilingual capabilities for demanding applications.	Keys	Text	API2.0
OpenAI SDK
gpt-5-2025-08-07	An August 7, 2025 build of GPTâ€‘5 featuring the latest training data and architectural improvements for state-of-the-art performance.	Keys	Text	API2.0
OpenAI SDK
gpt-5-mini	A smaller, efficient GPTâ€‘5 variant aimed at cost-effective deployment while inheriting many of GPTâ€‘5â€™s capabilities.	Keys	Text	API2.0
OpenAI SDK
gpt-5-mini-2025-08-07	An August 7, 2025 release of GPTâ€‘5 Mini with optimizations for responsiveness and typical assistant tasks.	Keys	Text	API2.0
OpenAI SDK
gpt-5-nano	An ultra-small GPTâ€‘5-based model targeted at extremely low-resource environments and simple, fast responses.	Keys	Text	API2.0
OpenAI SDK
gpt-5-nano-2025-08-07	An August 7, 2025 version of GPTâ€‘5 Nano with incremental quality improvements while remaining highly efficient.	Keys	Text	API2.0
OpenAI SDK
gpt-5.1	GPTâ€‘5.1 is an updated generation building on GPTâ€‘5, offering better reasoning reliability, safety, and domain coverage.	Keys	Text	API2.0
OpenAI SDK
gpt-5.1-2025-11-13	A November 13, 2025 snapshot of GPTâ€‘5.1 incorporating the latest fine-tuning and safety updates.	Keys	Text	API2.0
OpenAI SDK
gpt-5-chat-latest	The latest chat-focused configuration of GPTâ€‘5, providing an optimized conversational experience.	Keys	Text	API2.0
OpenAI SDK
gpt-5.1-chat-latest	The latest chat-optimized build of GPTâ€‘5.1, tuned for multi-turn dialogue, safety, and helpfulness.	Keys	Text	API2.0
OpenAI SDK
gpt-5-search-api	A GPTâ€‘5-based model optimized as a search API, designed for integrating search and retrieval into applications.	Keys	Text	API2.0
OpenAI SDK
gpt-5-search-api-2025-10-14	An October 14, 2025 build of the GPTâ€‘5 Search API with updated ranking, retrieval, and grounding behavior.	Keys	Text	API2.0
OpenAI SDK
gpt-4	GPTâ€‘4 is OpenAIâ€™s flagship model generation, delivering strong general-purpose performance across many tasks.	Keys	Text	API2.0
OpenAI SDK
gpt-4-0613	A June 13, 2023 build of GPTâ€‘4 with stable functionâ€‘calling and tool-use support widely used in production.	Keys	Text	API2.0
OpenAI SDK
gpt-4-1106-preview	A November 6, 2023 preview of GPTâ€‘4 featuring newer capabilities and experimental improvements.	Keys	Text	API2.0
OpenAI SDK
gpt-4-0125-preview	A January 25, 2024 GPTâ€‘4 preview with updated training data and enhanced reasoning in complex scenarios.	Keys	Text	API2.0
OpenAI SDK
gpt-4-turbo-preview	A preview version of GPTâ€‘4 Turbo providing early access to upcoming improvements in speed and capability.	Keys	Text	API2.0
OpenAI SDK
gpt-3.5-turbo	GPTâ€‘3.5â€‘turbo is a fast, cost-effective version of GPTâ€‘3.5, optimized for dialogue and general tasks with broad capabilities but less powerful than GPTâ€‘4.	Keys	Text	API2.0
OpenAI SDK
gpt-3.5-turbo-16k	A 16k-context version of GPTâ€‘3.5â€‘turbo, ideal for longer documents and extended conversations.	Keys	Text	API2.0
OpenAI SDK
gpt-3.5-turbo-1106	A November 6, 2023 build of GPTâ€‘3.5â€‘turbo with updates for reliability and cost efficiency.	Keys	Text	API2.0
OpenAI SDK
gpt-3.5-turbo-0125	A January 25, 2024 release of GPTâ€‘3.5â€‘turbo with improved instruction-following and robustness.	Keys	Text	API2.0
OpenAI SDK
DeepSeek models
Deepseek is a series of large language models developed by the company Deepseek, known for their strong performance in code generation, reasoning, and natural language tasks.

Model name	Description	Type	Modality	Endpoints
deepseek-r1	Deepseek-R1 is a specialized AI model by Deepseek, optimized for reasoning and code generation tasks, with strong performance in logical problem-solving and programming. Itâ€™s designed to handle complex, multi-step tasks efficiently.	Keys	Text	API2.0
OpenAI SDK
deepseek-v3	Deepseek-V3 is an advanced AI model by Deepseek, excelling in reasoning, code generation, and multi-step problem-solving. It builds on prior versions with enhanced capabilities for complex tasks and improved efficiency.	Keys	Text	API2.0
OpenAI SDK
Llama models
Llama is a series of open-source large language models developed by Meta. Known for their versatility and multilingual support, Llama models excel in natural language understanding, code generation, and reasoning tasks.

Model name	Description	Type	Modality	Endpoints
llama-3.3-8b	Llamaâ€‘3.3â€‘8B is a compact 8â€‘billionâ€‘parameter model optimized for efficient deployment with strong language understanding and coding support.	Keys	Text	API2.0
OpenAI SDK
llama-3.3-70b	Llamaâ€‘3.3â€‘70B is a larger, more capable variant suited for complex reasoning, coding, and multilingual tasks.	Keys	Text	API2.0
OpenAI SDK
llama-4-maverick	Llamaâ€‘4 Maverick is a next-gen Llama model designed for strong general-purpose performance and efficient deployment.	Keys	Text	API2.0
OpenAI SDK
llama-4-maverick-17b-128e-instruct	A 17B Llamaâ€‘4 Maverick instruct-tuned model with extended context (128k) for advanced reasoning and long-document tasks.	Keys	Text	API2.0
OpenAI SDK
llama-4-scout	Llamaâ€‘4 Scout is optimized for fast inference and exploratory tasks while maintaining strong language capabilities.	Keys	Text	API2.0
OpenAI SDK
llama-3-3-70b	Llamaâ€‘3.3â€‘70B offers high-end reasoning, coding, and multilingual support suitable for complex enterprise tasks.	Keys	Text	API2.0
OpenAI SDK
llama-3-1-8b	Llamaâ€‘3.1â€‘8B is an 8â€‘billionâ€‘parameter model focused on efficiency and high-quality language understanding for everyday workloads.	Keys	Text	API2.0
OpenAI SDK
llama-3.1	A provider-based Llamaâ€‘3.1 configuration, offering strong general-purpose performance and multilingual capabilities.	Provider	Text	API2.0
OpenAI SDK
Claude models
Claude is a series of large language models developed by Anthropic, known for their strong natural language understanding, reasoning, and code generation capabilities.

Model name	Description	Type	Modality	Endpoints
claude-3.5-sonnet	Claudeâ€‘3.5â€‘Sonnet is a powerful AI model by Anthropic, excelling in natural language tasks, reasoning, and code generation, with a focus on safety, accuracy, and handling complex, multi-step problems.	Provider	Text	API2.0
OpenAI SDK
Le Chat models
Mistral is a series of open-weight large language models developed by Mistral AI, known for their efficiency, scalability, and strong performance in natural language tasks, reasoning, and code generation.

Model name	Description	Type	Modality	Endpoints
mistral-small-3.1	Mistralâ€‘Smallâ€‘3.1 is a compact, efficient variant of the Mistral series, designed for cost-effective and fast performance on simpler tasks while maintaining strong language understanding and reasoning capabilities.	Keys	Text	API2.0
OpenAI SDK
e5-mistral-7b	E5â€‘Mistralâ€‘7B is a 7â€‘billionâ€‘parameter model optimized for embeddings and semantic understanding while still supporting general text tasks.	Keys	Text	API2.0
OpenAI SDK
Qwen models
Qwen is a series of large language models developed by Alibaba Cloud, designed for versatility and high performance across a wide range of tasks.

Model name	Description	Type	Modality	Endpoints
qwen-3-235b-a22b-instruct-2507	A large-scale Qwenâ€‘3 instruct model with 235B parameters (A22B), tuned for complex reasoning, long-context tasks, and precise instruction-following.	Keys	Text	API2.0
OpenAI SDK
qwen3-max-2025-10-30	Qwenâ€‘3 Max (October 30, 2025) is a highly capable general-purpose model suited for demanding workloads requiring advanced reasoning and coding.	Keys	Text	API2.0
OpenAI SDK
qwen3-vl-plus	Qwenâ€‘3 VL Plus is a vision-language model able to process and reason over both images and text.	Keys	Text	API2.0
OpenAI SDK
qwen3-vl-32b	A 32B-parameter Qwenâ€‘3 vision-language model, offering strong multimodal understanding and generation.	Keys	Text	API2.0
OpenAI SDK
qwen3-coder-plus	Qwenâ€‘3 Coder Plus is specialized for programming tasks, code generation, and debugging across multiple languages.	Keys	Text	API2.0
OpenAI SDK
qwen	Qwen is a large language model developed by Alibaba Cloud, excelling in natural language understanding, code generation, and multi-language support, with a focus on versatility and handling complex tasks efficiently.	Provider	Text	API2.0
OpenAI SDK
Google models
Gemini is a series of large language models developed by Google, known for their advanced reasoning, natural language understanding, and multimodal capabilities.

Model name	Description	Type	Modality	Endpoints
gemini-2.5-flash	A fast and efficient version of Google's Gemini model, optimized for real-time, low-latency tasks while maintaining strong multi-modal understanding.	Keys	Text	API2.0
OpenAI SDK
gemini-2.5-flash-lite-preview-06-17	A lightweight preview release of the Gemini Flash model from June 17, designed for cost-effective, high-speed inference on less complex tasks.	Keys	Text	API2.0
OpenAI SDK
gemini-2.0-flash	A fast, efficient model optimized for quick responses and cost-effective tasks while maintaining strong performance.	Keys	Text	API2.0
OpenAI SDK
gemini-2.0-flash-lite	A lighter version of gemini-2.0-flash, prioritizing speed and lower resource usage for simpler tasks.	Keys	Text	API2.0
OpenAI SDK
gemini-1.5-flash	A fast and capable model from the 1.5 series, designed for general-purpose tasks with a balance of speed and accuracy.	Keys	Text	API2.0
OpenAI SDK
gemini-1.5-flash-8b	A smaller, 8-billion-parameter variant of gemini-1.5-flash, offering faster inference for less complex tasks.	Keys	Text	API2.0
OpenAI SDK
gemini-1.5-pro	A high-performance model in the 1.5 series, optimized for complex, multi-step tasks requiring advanced reasoning and precision.	Keys	Text	API2.0
OpenAI SDK
gemma-3-27b-it	gemma-3-27b-it is a 27â€‘billionâ€‘parameter Gemma model fine-tuned for instruction following, delivering strong performance on complex language and coding tasks.	Keys	Text	API2.0
OpenAI SDK
gemma-3-4b-it	gemma-3-4b-it is a 4â€‘billionâ€‘parameter model from the Gemma series, optimized for instructional tasks (IT). It provides strong performance in natural language understanding and task-specific applications, balancing efficiency and capability.	Keys	Text	API2.0
OpenAI SDK
gemma-2-2b-it	A 2â€‘billionâ€‘parameter Gemmaâ€‘2 model tuned for instruction following, ideal for lightweight applications with limited resources.	Keys	Text	API2.0
OpenAI SDK
gemma-2-9b-it-fast	A 9â€‘billionâ€‘parameter Gemmaâ€‘2 model optimized for fast inference while maintaining strong instruction-following performance.	Keys	Text	API2.0
OpenAI SDK
Cohere models
Command is a specialized AI model developed by Cohere, designed for understanding and executing commands or instructions effectively. It focuses on task-specific applications, such as interpreting user inputs, automating workflows, and performing actions based on natural language commands.

Model name	Description	Type	Modality	Endpoints
command-a-03-2025	A March 2025 iteration of the Command model, optimized for advanced text generation, reasoning, and conversational tasks.	Keys	Text	API2.0
OpenAI SDK
command-r7b-12-2024	A December 2024 Commandâ€‘R7B model focusing on reasoning and instruction execution with improved robustness.	Keys	Text	API2.0
OpenAI SDK
command-r-plus-04-2024	An April 2024 enhanced variant of Commandâ€‘R+, designed for superior reasoning and task-specific applications.	Keys	Text	API2.0
OpenAI SDK
command-r-plus	An advanced version of the Commandâ€‘R model, emphasizing improved reasoning and execution capabilities.	Keys	Text	API2.0
OpenAI SDK
command-r-08-2024	An August 2024 release of the Commandâ€‘R model, optimized for interpreting and executing commands.	Keys	Text	API2.0
OpenAI SDK
command-r-03-2024	A March 2024 release of the Commandâ€‘R model, focused on command-driven tasks and automation.	Keys	Text	API2.0
OpenAI SDK
command-r	The standard Commandâ€‘R model, tailored for understanding and executing instructions effectively.	Keys	Text	API2.0
OpenAI SDK
command	The foundational Command model, designed for general-purpose text generation, conversations, and question-answering.	Keys	Text	API2.0
OpenAI SDK
command-nightly	A nightly build of the Command model, offering the latest experimental features and updates.	Keys	Text	API2.0
OpenAI SDK
command-light	A lightweight version of the Command model, optimized for speed and efficiency in resource-constrained environments.	Keys	Text	API2.0
OpenAI SDK
command-light-nightly	A nightly build of the lightweight Command model, providing rapid updates and experimental features for lightweight use cases.	Keys	Text	API2.0
OpenAI SDK
c4ai-aya-expanse-32b	A 32â€‘billionâ€‘parameter model developed by C4AI, designed for expansive reasoning, complex tasks, and high-performance applications.	Keys	Text	API2.0
OpenAI SDK
Other models
Additional providers and specialized models available via OnlySq.

Model name	Description	Type	Modality	Endpoints
phi-4	Phiâ€‘4 is a compact, efficient model known for strong reasoning-to-size ratio, ideal for low-cost deployments and experimentation.	Keys	Text	API2.0
OpenAI SDK
searchgpt	A specialized GPT-based configuration optimized for search and retrieval applications.	Keys	Text	API2.0
OpenAI SDK
grok	Grok is an AI model focused on real-time, internet-aware conversations and reasoning, suited for up-to-date question answering.	Keys	Text	API2.0
OpenAI SDK
evil	An experimental model configuration intended for testing robustness and safety systems against adversarial or â€œjailbreakâ€ prompts.	Keys	Text	API2.0
OpenAI SDK
mirexa	Mirexa is a custom model focused on dialogue and creative writing tasks.	Keys	Text	API2.0
OpenAI SDK
gpt-oss-120b	GPTâ€‘OSSâ€‘120B is an open-source style 120â€‘billionâ€‘parameter model providing high-end reasoning and generation capabilities.	Keys	Text	API2.0
OpenAI SDK
zai-glm-4.6	ZAI GLMâ€‘4.6 is a high-capacity model in the GLM family, designed for complex reasoning, multilingual support, and coding.	Keys	Text	API2.0
OpenAI SDK
zai-glm-4-5	ZAI GLMâ€‘4.5 is a previous-generation GLM model offering strong general-purpose language and coding performance.	Keys	Text	API2.0
OpenAI SDK
zai-glm-4-5-air	A lightweight â€œAirâ€ variant of ZAI GLMâ€‘4.5, focused on speed and lower resource usage while keeping good quality.	Keys	Text	API2.0
OpenAI SDK
kimi-k2	Kimiâ€‘K2 is a model focused on multilingual chat and reasoning, with strong performance on practical assistant tasks.	Keys	Text	API2.0
OpenAI SDK
c4ai-aya-expanse-32b	A 32â€‘billionâ€‘parameter Aya Expanse model from C4AI, designed for large-scale reasoning, multilingual understanding, and complex tasks.	Keys	Text	API2.0
OpenAI SDK
Image models
Image Models are AI models designed for tasks involving images, such as recognition, segmentation, generation, enhancement, and style transfer.

Model name	Description	Type	Modality	Endpoints
flux	Flux is a 12 billion parameter rectified flow transformer capable of generating images from text descriptions.	Keys	Images	ImaGen
Sound models
Sound models are designed to convert text to speech and handle audio-focused tasks.

Model name	Description	Type	Modality	Endpoints
gtts	gTTS (Google Text-to-Speech) generates natural-sounding speech audio from text in multiple languages.	Provider	Sound	TTS
Pager
Previous page
Get started
Next page
Tools overview


Skip to content

OnlySq Documentation
Main Navigation
Home

API
All models
About us

Sidebar Navigation
OnlySq API
About

Get started

All models

Tools
Tools overview

Tool calling

Tool building

Image understanding

Endpoints
SDK OpenAI

POST API2.0

POST ImaGen

BETA OnlySq Cloud
How to work with Cloud?

On this page
How it works
Supported languages
Tool Building
Tool Object Structure
Example: Getting the weather
Tools
The OnlySq API is compatible with the OpenAI SDK, allowing you to leverage powerful tool-calling capabilities. This feature enables the model to interact with external functions to perform actions or retrieve information.

How it works
The process involves a few steps:

Define tools: You provide the model with a list of available functions (tools) it can use.
User request: The user sends a message that requires external information.
Model decides: The model analyzes the user's request and decides if a tool is needed. If so, it generates a tool_calls object instead of a direct response.
Execute the tool: Your application receives the tool_calls object, parses it, and executes the specified function with the provided arguments.
Provide feedback: The result from the function is sent back to the model as a new message with the role: "tool".
Final response: The model processes the tool's output and generates a final, human-readable response for the user.
Supported languages
TypeScript / JavaScript
Python
Java
.NET
Go
Tool Building
To use tools, you must first define them in a specific format that the model understands. A tool is a dictionary with type and function keys.

type: Must be "function".
function: A dictionary describing the function.
name: The name of the function to be called.
description: A human-readable description of what the function does. This helps the model decide when to use it.
parameters: An object defining the function's arguments in JSON Schema format. This is crucial for the model to know what arguments to provide.
Tool Object Structure
Here's a template for defining a tool:


{
    "type": "function",
    "function": {
        "name": "your_function_name",
        "description": "A description of what your function does.",
        "parameters": {
            "type": "object",
            "properties": {
                "parameter1": {
                    "type": "string",
                    "description": "Description of the first parameter."
                },
                "parameter2": {
                    "type": "number",
                    "description": "Description of the second parameter."
                }
            },
            "required": ["parameter1"]
        }
    }
}
Example: Getting the weather
Let's look at a complete example using a weather function.

First, define the get_weather function and the corresponding tool definition.


import json
import requests
from openai import OpenAI

# 1. Define the external function
def get_weather(latitude: float, longitude: float) -> float:
    """Get current temperature for provided coordinates in celsius."""
    response = requests.get(
        f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m"
    )
    data = response.json()
    return data['current']['temperature_2m']

# 2. Define the tool for the model
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get the current weather for a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "latitude": { "type": "number" },
                    "longitude": { "type": "number" },
                },
                "required": ["latitude", "longitude"],
            },
        },
    }
]

client = OpenAI(base_url="https://api.onlysq.ru/ai/openai", api_key="openai")

# 3. Initial user message
input_messages = [{"role": "user", "content": "What's the weather like in Paris today? 48Â°51â€²24â€³N 2Â°21â€²8â€³E"}]

# 4. First API call: The model will "call" the tool
response = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=input_messages,
    tools=tools,
    tool_choice="auto"
)

# 5. Extract and execute the tool call
tool_call = response.choices[0].message.tool_calls[0]
function_name = tool_call.function.name
function_args = json.loads(tool_call.function.arguments)

if function_name == "get_weather":
    result = get_weather(
        latitude=function_args.get("latitude"),
        longitude=function_args.get("longitude")
    )
else:
    result = "Unknown function called."

# 6. Append the model's tool call and the tool's output to the messages
input_messages.append(response.choices[0].message)
input_messages.append(
    {
        "role": "tool",
        "tool_call_id": tool_call.id,
        "name": function_name,
        "content": str(result),
    }
)

# 7. Second API call: The model generates a final response based on the tool's output
response_2 = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=input_messages,
    tools=tools,
)

print(response_2.choices[0].message.content)
Pager
Previous page
All models
Next page
Tool calling


Skip to content

OnlySq Documentation
Main Navigation
Home

API
All models
About us

Sidebar Navigation
OnlySq API
About

Get started

All models

Tools
Tools overview

Tool calling

Tool building

Image understanding

Endpoints
SDK OpenAI

POST API2.0

POST ImaGen

BETA OnlySq Cloud
How to work with Cloud?

On this page
tool_calls Object
Example tool_calls response
tool_choice Parameter
Streaming Tool Calls
Python Example with Streaming
Tool Calling
Tool calling is the process by which a model determines whether to use one or more of the functions you've provided. When a user's prompt can be better answered by calling a function, the model responds with a tool_calls object instead of a text message.

tool_calls Object
The tool_calls object is a list of calls the model wants to make. Each item in the list contains:

id: A unique identifier for the tool call. This is required for the subsequent tool message.
type: The type of tool, always "function".
function: An object with the function details.
name: The name of the function to call.
arguments: A string containing the JSON-formatted arguments for the function.
Example tool_calls response

{
    "id": "chat_XBt6z670WKm7L9BVoCcLZLzTNZ03UJhD6sWqAEUyTgaJvhJA",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "tool_calls": [
                    {
                        "id": "call_abc123",
                        "type": "function",
                        "function": {
                            "name": "get_weather",
                            "arguments": "{\"latitude\":48.8566,\"longitude\":2.3522}"
                        }
                    }
                ]
            },
            "finish_reason": "tool_calls"
        }
    ],
    // ... other fields
}
tool_choice Parameter
You can control the model's behavior with the tool_choice parameter in the API request.

none (default): The model will not call any functions.
auto: The model decides whether to call a function or generate a text response.
required: The model must call one or more functions.

# Example of using tool_choice
response = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=input_messages,
    tools=tools,
    tool_choice="auto" # or "none", or "required"
)
Streaming Tool Calls
When using stream=True, the tool_calls object will be returned in chunks. The delta object in each chunk contains the partial tool_calls data.

Python Example with Streaming
This example demonstrates how to handle streaming tool calls and their results.


import json
import requests
from openai import OpenAI

def get_weather(latitude: float, longitude: float):
    response = requests.get(
        f"https://api.open-meteo.com/v1/forecast?latitude={latitude}&longitude={longitude}&current=temperature_2m,wind_speed_10m&hourly=temperature_2m,relative_humidity_2m,wind_speed_10m"
    )
    data = response.json()
    return data['current']['temperature_2m']

client = OpenAI(base_url="https://api.onlysq.ru/ai/openai", api_key="openai")

tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get the current weather for a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "latitude": { "type": "number" },
                    "longitude": { "type": "number" },
                },
                "required": ["latitude", "longitude"],
            },
        },
    }
]

input_messages = [{"role": "user", "content": "What's the weather like in Paris today? 48Â°51â€²24â€³N 2Â°21â€²8â€³E"}]

# First API call to get the tool call
response = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=input_messages,
    tools=tools,
    tool_choice="auto"
)

if response.choices[0].message.tool_calls:
    tool_call = response.choices[0].message.tool_calls[0]
    function_name = tool_call.function.name
    function_args = json.loads(tool_call.function.arguments)

    if function_name == "get_weather":
        # Execute the tool
        result = get_weather(
            latitude=function_args.get("latitude"),
            longitude=function_args.get("longitude")
        )
        
        # Append the tool's output to messages
        input_messages.append(response.choices[0].message)
        input_messages.append(
            {
                "role": "tool",
                "tool_call_id": tool_call.id,
                "name": function_name,
                "content": str(result),
            }
        )

        # Second API call with streaming enabled
        stream = client.chat.completions.create(
            model="gemini-2.0-flash",
            messages=input_messages,
            tools=tools,
            stream=True
        )
        
        # Process and print the streaming response
        for chunk in stream:
            content = chunk.choices[0].delta.content
            if content:
                print(content, end="", flush=True)
        print()
    else:
        print("Unknown function called.")
else:
    print(response.choices[0].message.content)
Pager
Previous page
Tools overview
Next page
Tool building


Skip to content

OnlySq Documentation
Main Navigation
Home

API
All models
About us

Sidebar Navigation
OnlySq API
About

Get started

All models

Tools
Tools overview

Tool calling

Tool building

Image understanding

Endpoints
SDK OpenAI

POST API2.0

POST ImaGen

BETA OnlySq Cloud
How to work with Cloud?

On this page
Best Practices
Example: A Tool with Multiple Parameters and a Complex Response
Tool Definition
Python Implementation
Tool Building
Building effective tools is key to unlocking the full potential of your AI assistant. This guide covers the best practices for defining your tools and their parameters.

Best Practices
Clear and Descriptive Names: Use clear, concise, and descriptive names for your functions and parameters. A name like get_current_weather is better than get_weather.
Detailed Descriptions: Provide a clear description for both the function and its parameters. The model relies heavily on these descriptions to understand when and how to use your tool. A good description explains the function's purpose, what it returns, and any important context.
Accurate Parameters: Define your parameters using JSON Schema. This ensures the model provides the correct data types (string, number, boolean, array, object). Be precise with the required fields.
Error Handling: Your application code should handle cases where a tool call fails. Return meaningful error messages in the content of the role: "tool" message so the model can inform the user appropriately.
Example: A Tool with Multiple Parameters and a Complex Response
Tool Definition
Let's imagine a tool that searches a product database. This tool requires a search query, and has optional parameters for filtering.


{
    "type": "function",
    "function": {
        "name": "search_products",
        "description": "Searches the product database for items matching a specific query. Can filter by price range and category.",
        "parameters": {
            "type": "object",
            "properties": {
                "query": {
                    "type": "string",
                    "description": "The search term for the product, e.g., 'smartphone'."
                },
                "min_price": {
                    "type": "number",
                    "description": "The minimum price of the product."
                },
                "max_price": {
                    "type": "number",
                    "description": "The maximum price of the product."
                },
                "category": {
                    "type": "string",
                    "description": "The product category, e.g., 'electronics' or 'clothing'."
                }
            },
            "required": ["query"]
        }
    }
}
Python Implementation
Here's how you might implement this in Python.


import json
from openai import OpenAI

# 1. Define the tool
tools = [
    {
        "type": "function",
        "function": {
            "name": "search_products",
            "description": "Searches the product database for items matching a specific query. Can filter by price range and category.",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "The search term for the product, e.g., 'smartphone'."
                    },
                    "min_price": {
                        "type": "number",
                        "description": "The minimum price of the product."
                    },
                    "max_price": {
                        "type": "number",
                        "description": "The maximum price of the product."
                    },
                    "category": {
                        "type": "string",
                        "description": "The product category, e.g., 'electronics' or 'clothing'."
                    }
                },
                "required": ["query"]
            }
        }
    }
]

# 2. Define the mock function to simulate database search
def search_products(query: str, min_price: float = None, max_price: float = None, category: str = None) -> list:
    """A mock function to simulate a product search."""
    # In a real application, this would query a database
    mock_db = [
        {"name": "Laptop Pro", "price": 1200, "category": "electronics"},
        {"name": "Gaming Mouse", "price": 75, "category": "electronics"},
        {"name": "T-shirt", "price": 25, "category": "clothing"},
        {"name": "Fancy Phone", "price": 950, "category": "electronics"}
    ]
    
    results = [p for p in mock_db if query.lower() in p["name"].lower()]
    if min_price:
        results = [p for p in results if p["price"] >= min_price]
    if max_price:
        results = [p for p in results if p["price"] <= max_price]
    if category:
        results = [p for p in results if p["category"].lower() == category.lower()]
        
    return results

client = OpenAI(base_url="https://api.onlysq.ru/ai/openai", api_key="openai")

# 3. User prompt
input_messages = [{"role": "user", "content": "I'm looking for an electronic device that costs less than $1000."}]

# 4. First API call
response = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=input_messages,
    tools=tools,
    tool_choice="auto"
)

# 5. Execute the tool call
tool_call = response.choices[0].message.tool_calls[0]
function_name = tool_call.function.name
function_args = json.loads(tool_call.function.arguments)

if function_name == "search_products":
    search_results = search_products(
        query=function_args.get("query", ""),
        min_price=function_args.get("min_price"),
        max_price=function_args.get("max_price"),
        category=function_args.get("category")
    )
    
    result_content = json.dumps(search_results)
else:
    result_content = "Unknown function called."
    
# 6. Append the tool's output
input_messages.append(response.choices[0].message)
input_messages.append(
    {
        "role": "tool",
        "tool_call_id": tool_call.id,
        "name": function_name,
        "content": result_content,
    }
)

# 7. Final response
response_2 = client.chat.completions.create(
    model="gemini-2.0-flash",
    messages=input_messages,
    tools=tools,
)

print(response_2.choices[0].message.content)
Pager
Previous page
Tool calling
Next page
Image understanding

Skip to content

OnlySq Documentation
Main Navigation
Home

API
All models
About us

Sidebar Navigation
OnlySq API
About

Get started

All models

Tools
Tools overview

Tool calling

Tool building

Image understanding

Endpoints
SDK OpenAI

POST API2.0

POST ImaGen

BETA OnlySq Cloud
How to work with Cloud?

Image understanding
Note: Currently, recognition only works for the Gemini AI family.

The OnlySq API provides the ability to recognize and process images along with a text query - it's easy to use:


from openai import OpenAI
import base64

def encode_image(image_path):
	with open(image_path, "rb") as image_file:
	    return base64.b64encode(image_file.read()).decode('utf-8')

client = OpenAI(
    base_url="https://api.onlysq.ru/ai/openai",
    api_key="openai",
)

base = encode_image('pic.jpg')

completion = client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=[
        {
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": "What on this image?",
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url":  f"data:image/jpeg;base64,{base}"
                    },
                },
            ],
        },
    ],
)

print(completion.choices[0].message.content)
It also works well for API 2.0:


import requests
import base64

def encode_image(image_path):
	with open(image_path, "rb") as image_file:
	    return base64.b64encode(image_file.read()).decode('utf-8')

base = encode_image('pic.jpg')

send = {
    "model": "gemini-2.5-flash",
    "request": {
        "messages": [
            {
                "role": "user",
                "content": [
	                {
	                    "type": "text",
	                    "text": "What on this image?",
	                },
	                {
	                    "type": "image_url",
	                    "image_url": {
	                        "url":  f"data:image/jpeg;base64,{base}"
	                    },
	                },
	            ],
            }
        ]
    }
}

request = requests.post('http://api.onlysq.ru/ai/v2', json=send)
response = request.json()
print(response["choices"][0]["message"]["content"])
How it works
First, we convert image to base64:


def encode_image(image_path):
  with open(image_path, "rb") as image_file:
    return base64.b64encode(image_file.read()).decode('utf-8')

base = encode_image('pic.jpg')
Then, we insert the formatted string into the query:


{
    "type": "image_url", # Say that we are sending the image
    "image_url": {
        "url":  f"data:image/jpeg;base64,{base}"
    },
},
Finally, we send formatted request to API.

The API processes the base64 string and sends an image to AI model, after - response is returned to the user.

Response example
This image features a kitten in the snow, looking at a series of paw prints.

Here's a breakdown of what's visible:

Snow: The ground is covered in white, fluffy snow.
Kitten: A small, striped (tabby) kitten with gray and white fur is visible from a top-down, slightly backward angle. It has some snowflakes on its fur, particularly on its head and back.
Paw Prints: Several distinct paw prints are embedded in the snow, leading away from the kitten. They appear to be cat paw prints.
pic.jpg:Output image

Pager
Previous page
Tool building
Next page
SDK OpenAI


Skip to content

OnlySq Documentation
Main Navigation
Home

API
All models
About us

Sidebar Navigation
OnlySq API
About

Get started

All models

Tools
Tools overview

Tool calling

Tool building

Image understanding

Endpoints
SDK OpenAI

POST API2.0

POST ImaGen

BETA OnlySq Cloud
How to work with Cloud?

On this page
Supported languages
Installation
Initializing client
Chat completions
State management
Supported parameters
Streaming chat completions usage
Streaming request example
Streaming chunk example
Using OnlySq AI models via OpenAI SDK
The API is compatible for calls from OpenAI libraries. This allows you to easily connect projects to the OnlySq API, use any of the 40+ available models for free, use it in projects such as Exteragram, etc.

Supported languages
TypeScript / JavaScript
Python
Java
.NET
Go
This short course will show you how to use OnlySq API via OpenAI SDK.

Installation
First you need to install the required dependencies and import SDK Then, create a client and configure it with the OnlySq API URL and yours or a universal key openai.


Python

TypeScript

pip install openai
Initializing client

Python

TypeScript

from openai import OpenAI

client = OpenAI(
    base_url="https://api.onlysq.ru/ai/openai/",
    api_key="openai" # or your valid key
)
Chat completions
Hereâ€™s a basic example of using Chat Completions


Python

TypeScript

cURL

from openai import OpenAI

client = OpenAI(
    base_url="https://api.onlysq.ru/ai/openai",
    api_key="openai",
)

completion = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[
        {
            "role": "user",
            "content": "Say 5 short facts about AI.",
        },
    ],
)

print(completion.choices[0].message.content)
Example response (via the Python SDK):


Here are five short facts about AI:  

1. **AI Learns from Data** â€“ AI systems improve by analyzing large amounts of data, identifying patterns, and making predictions.          
2. **Narrow vs. General AI** â€“ Most AI today is "narrow" (task-specific), while "general AI" (human-like reasoning) is still theoretical.  
3. **AI Powers Everyday Tech** â€“ Virtual assistants (Siri, Alexa), recommendations (Netflix, Spotify), and spam filters all use AI.        
4. **Ethical Concerns Exist** â€“ AI raises issues like bias in algorithms, job displacement, and privacy risks.  
5. **AI is Evolving Fast** â€“ Breakthroughs in deep learning and generative AI (like ChatGPT) are rapidly advancing the field.  

Would you like more details on any of these?
State management
For state management, you can use the messages parameter to build the conversation history. You can include a system message via the system (or developer) role and the multiple chat turns between the user and assistant.


Python

TypeScript

from openai import OpenAI

client = OpenAI(
    base_url="https://api.onlysq.ru/ai/openai",
    api_key="openai",
)

completion = client.chat.completions.create(
    model="gpt-4o-mini"
    messages=[
        {
            "role": "system",
            "content": "You are a neko-helper.",
        },
        {
            "role": "user",
            "content": "What's 5 + 5?",
        },
        {
            "role": "assistant",
            "content":"Nya~! 5 + 5 is 10, purr-fectly simple! ðŸ˜¸âœ¨"
        },
        {
            "role": "user",
            "content": "Say what do you like",
        },
    ],
)

print(completion.choices[0].message.content)
Example response (via the Python SDK):


Nya~! As your neko-helper, I love all things cute and cozyâ€”like chasing digital yarn balls, napping in the cloud (servers!), and most of all... helping you with purr-oblems! ðŸ˜»âœ¨  

What about you? What do *you* like, nya? ~(=^ï½¥Ï‰ï½¥^)ï¾‰
Supported parameters
model - required
messages - required
Streaming chat completions usage
Streaming request example

from openai import OpenAI
client = OpenAI(api_key="openai", base_url="https://api.onlysq.ru/ai/openai/")
messages = [
    {
        "role":"user",
        "content":"Write a one-line story about AI."
    }
]
r = client.chat.completions.create(
    model = "gpt-4o-mini",
    messages = messages,
    stream = True
)

for chunk in r:
    print(chunk.choices[0].delta.content, end="")
This returns the following response to the console:


"After mastering human emotions, the AI sighed and shut itself down, realizing loneliness wasnâ€™t worth the code."
Streaming chunk example

{
    "id": "chatcmpl_lQdhTfXn4yKoDlyCjuBpL0GfPZNonZufGqOyGl3wVpMhJRwP",
    "object": "chat.completion.chunk",
    "created": 1750612428,
    "model": "gpt-4o-mini",
    "choices": [
        {
            "index": 0,
            "delta": {
                "content": "te",
                "role": "assistant"
            },
            "finish_reason": null
        }
    ],
    "usage": null
}
Pager
Previous page
Image understanding
Next page
POST API2.0

Skip to content

OnlySq Documentation
Main Navigation
Home

API
All models
About us

Sidebar Navigation
OnlySq API
About

Get started

All models

Tools
Tools overview

Tool calling

Tool building

Image understanding

Endpoints
SDK OpenAI

POST API2.0

POST ImaGen

BETA OnlySq Cloud
How to work with Cloud?

On this page
Request
Response
Examples
Sync request example
Response example
Streaming request example
Streaming chunk example
POST https://api.onlysq.ru/ai/v2

Generates a text response to a user message(s).

Starting with patch 4.0.1 beta (November 29, 2025), every request to v2/imagen must have Authorization header


{"Authorization": "Bearer apikey"}
Basic API key is openai

Request
This endpoint expects an object.

model string Required

The name of a OnlySq AI model which will process the request

request dict Required

Query data dictionary.

Contains:

messages list Required

A list of chat messages in chronological order, representing a conversation between the user and the model. Messages can be from User, Assistant and System roles.

meta dict Optional

An optional dictionary previously used by ImaGen models.

Contains:

image_count int Optional

Optional parameter, previously used when generating with ImaGen

Response
id string

Unique identifier for the generated reply. Useful for submitting feedback.

object string

Object initialization for OpenAI SDK.

created int

Timestamp in UNIX format.

model string

The model that processed the request.

answer string Disabled

The old version of receiving a response. Will be completely removed on 05/01/2025

choices list

The list of completion choices the model generated for the input prompt.

Contains:

index integer

Index of generated response

message dict

An object containing the response of the model that processed the request

Contains:

role string

A role of message, can be one of: User, Assistant, System.

content string

A response text.

finish_reason string

The reason a chat request has finished.

stop: The model finished sending a complete message.
error: The generation failed due to an internal error.
usage dict

Contains:

prompt_tokens int

The number of billed input tokens.

completion_tokens int

The number of billed output tokens.

total_tokens int

Total number of billed tokens.

user int

Unique of user by API key. Useful for submitting feedback.

Examples
Sync request example

import requests

headers = {
    "Authorization":"Bearer openai"
}

send = {
    "model": "gemini-2.5-flash",
    "request": {
        "messages": [
            {
                "role": "user",
                "content": "Hi! Write a short one-line story"
            }
        ]
    }
}

request = requests.post('http://api.onlysq.ru/ai/v2', json=send, headers=headers)
response = request.json()
print(response)
This returns the following response to the console:


"Under the streetlight, she found the courage to let go of the letterâ€”and the past."
Response example

{
    "id": "chat_XBt6z670WKm7L9BVoCcLZLzTNZ03UJhD6sWqAEUyTgaJvhJA",
    "object": "chat.completion",
    "created": 1745146202,
    "model": "gpt-4o-mini",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "\"Under the full moon, the werewolf realized he'd forgotten his keysâ€”again.\"",
                "refusal": null,
                "annotations": []
            },
            "finish_reason": "stop"
        }
    ],
    "usage": {
        "prompt_tokens": 11,
        "completion_tokens": 19,
        "total_tokens": 30
    },
    "user": 0
}
Streaming request example

import requests, json

url = "http://api.onlysq.ru/ai/v2"

data = {
    "model": "gpt-4o-mini",
    "request": {
        "messages": [
            {
                "role": "user",
                "content": "Write a one-line story about AI."
            }
        ],
        "stream": True
    }
}

with requests.post(url, json=data, stream=True) as response:
    if response.status_code == 200:
        for line in response.iter_lines():
            if line:
                try:
                    decoded_line = line.decode('utf-8').strip()
                    if decoded_line.startswith("data: "):
                        decoded_line = decoded_line[len("data: "):]
                    if decoded_line == "[DONE]":
                        break
                    chunk = json.loads(decoded_line)
                    content = chunk["choices"][0]["delta"].get("content")
                    if content:
                        print(content, end="", flush=True)
                except Exception as e:
                    print(e)
    else:
        print(response.status_code, response.text)
This returns the following response to the console:


"After mastering human emotions, the AI sighed and shut itself down, realizing loneliness wasnâ€™t worth the code."
Streaming chunk example

{
    "id": "chatcmpl_lQdhTfXn4yKoDlyCjuBpL0GfPZNonZufGqOyGl3wVpMhJRwP",
    "object": "chat.completion.chunk",
    "created": 1750612428,
    "model": "gpt-4o-mini",
    "choices": [
        {
            "index": 0,
            "delta": {
                "content": "te",
                "role": "assistant"
            },
            "finish_reason": null
        }
    ],
    "usage": null
}
Pager
Previous page
SDK OpenAI
Next page
POST ImaGen


Skip to content

OnlySq Documentation
Main Navigation
Home

API
All models
About us

Sidebar Navigation
OnlySq API
About

Get started

All models

Tools
Tools overview

Tool calling

Tool building

Image understanding

Endpoints
SDK OpenAI

POST API2.0

POST ImaGen

BETA OnlySq Cloud
How to work with Cloud?

On this page
Request
Response
Examples
Request examples
Response example
POST https://api.onlysq.ru/ai/imagen

Generates an image based on the user's description.

Starting with patch 4.0.1 (November 29, 2025), every request to v2/imagen must have Authorization header


{"Authorization": "Bearer apikey"}
Basic API key is openai

Request
This endpoint expects an object.

model string Required

The name of a OnlySq AI model which will process the request

prompt string Required

A request to the model describing items, landscapes, and other elements that the user wishes to see in the image.

ratio string Optional

Aspect ratio of the requested image. Must be one of: 1:1, 16:9, 21:9, 3:2, 2:3, 4:5, 5:4, 3:4, 4:3, 9:16, 9:21.

Default: 1:1

Response
id string

Unique identifier for the generated reply. Useful for submitting feedback.

created int

Timestamp in UNIX format.

model string

The model that processed the request.

files list

The list contains images encoded in base64 utf-8 strings.

size dict

Contains:

width int

Count of width pixels.

height int

Count of height pixels.

elapsed-time float

Time spent on generation. Can be translated into another format, such as elapsed_time:.2f

usage int

Improvised token counter based on image height and width.

user int

Unique of user by API key. Useful for submitting feedback.

Examples
Request examples
The simplest request example:


import requests, base64
h = {"Authorization": "Bearer openai"}
j = {
	"model": "flux",
	"prompt": "cat on a blue water wave",
	"ratio":"16:9"
}

r = requests.post("https://api.onlysq.ru/ai/imagen", json=j, headers=h)
r.raise_for_status()
c = r.json()

with open('pic.png', 'wb') as f:
	f.write(base64.b64decode(c["files"][0]))

print("Image saved successfully")
Executing one of the presented examples will save the image of the cat to the file pic.png.

Response example
Output image


{
    "id": "imagen_u1nW43V9wAaWenrHifdqkzxSLNWIpXLVBMzJ4fnBsyNMx94w",
    "created": 1753881206,
    "model": "flux",
    "files": ["base64"],
    "ratio": "16:9",
    "elapsed-time": 3.20550274848938,
    "usage": 152,
    "user": 0
}
Pager
Previous page
POST API2.0
Next page
How to work with Cloud?